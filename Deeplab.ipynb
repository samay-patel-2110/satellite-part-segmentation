{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "from torchvision.io import ImageReadMode\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.transforms import InterpolationMode as IM\n",
    "import os\n",
    "import numpy as np\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MPS device ASSIGNED!.\n"
     ]
    }
   ],
   "source": [
    "# Loading Model to MPS Backend\n",
    "if torch.backends.mps.is_available():\n",
    "    mps_device = torch.device(\"mps\")\n",
    "    print(\"MPS device ASSIGNED!.\")\n",
    "else:\n",
    "    mps_device = torch.device('cpu')\n",
    "    print(\"MPS device not found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DeepLab V3\n",
    "\n",
    "Inverted convolution\n",
    "Squeeze adn excitation\n",
    "Depthwise-seperable Conv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SqueezeAndExcitation(nn.Module):\n",
    "    def __init__(self, c, ratio=8):\n",
    "        super().__init__()\n",
    "        self.ratio = ratio\n",
    "        self.L1 = nn.Linear(c, c // self.ratio)\n",
    "        self.L2 = nn.Linear(c // self.ratio, c)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        b = inputs.size(0)\n",
    "        c = inputs.size(1)\n",
    "        x = F.adaptive_avg_pool2d(inputs, 1)  # (batch_size, channels)\n",
    "        # device=self.device\n",
    "        x = self.L1(x.view(b, c))\n",
    "        x = F.relu(x)\n",
    "        # device=self.device\n",
    "        x = self.L2(x)\n",
    "        x = torch.sigmoid(x)\n",
    "        x = x.view(b, c, 1, 1)\n",
    "        x = F.relu(inputs * x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BlockResNet(nn.Module):\n",
    "    def __init__(self, input_c, output_c, device, kernel_size=3, K=1, stride=1, dilation=1, use_bias=True, depth=True):\n",
    "        super(BlockResNet, self).__init__()\n",
    "        self.k_s = kernel_size\n",
    "        self.dia = dilation\n",
    "        self.s = stride\n",
    "        self.depth = depth\n",
    "        self.device = device\n",
    "        self.out = output_c\n",
    "        # For stride = 1\n",
    "        \n",
    "\n",
    "        # For Stride = 2\n",
    "        if stride == 2:\n",
    "            self.conv_2 = nn.Conv2d(in_channels=input_c, out_channels=input_c*2, kernel_size=1,\n",
    "                                    stride=1, padding='same', groups=1)\n",
    "            if (dilation != 1):\n",
    "                self.depthwise_2 = nn.Conv2d(in_channels=input_c*2, out_channels=K*input_c*2,\n",
    "                                            kernel_size=kernel_size, padding=1, stride=1,\n",
    "                                            dilation=dilation, groups=input_c*2)\n",
    "            else:\n",
    "                self.depthwise_2 = nn.Conv2d(in_channels=input_c*2, out_channels=K*input_c*2,  # padding =1 will lead to half the size\n",
    "                                            kernel_size=kernel_size, padding=1, stride=stride,\n",
    "                                            dilation=1, groups=input_c*2)\n",
    "            self.pointwise_concat = nn.Conv2d(in_channels=input_c, out_channels=output_c,\n",
    "                                            kernel_size=1, padding='valid', stride=2,\n",
    "                                            groups=1)\n",
    "            self.pointwise_comp_2 = nn.Conv2d(in_channels=K*input_c*2, out_channels=output_c,\n",
    "                                        kernel_size=1, padding='same', stride=1,\n",
    "                                        groups=1)\n",
    "            self.batchn21 = nn.BatchNorm2d(input_c*2, momentum=0.001)\n",
    "            self.batchn22 = nn.BatchNorm2d(input_c*2, momentum=0.001)\n",
    "            self.batchn23 = nn.BatchNorm2d(output_c, momentum=0.001)\n",
    "            self.batchn24 = nn.BatchNorm2d(output_c, momentum=0.001)\n",
    "            self.SqueezeAndExcitation = SqueezeAndExcitation(output_c)\n",
    "\n",
    "        else:  \n",
    "            self.pointwise_exp = nn.Conv2d(in_channels=input_c, out_channels=input_c*2,\n",
    "                                       kernel_size=1, padding='same', stride=1,\n",
    "                                       dilation=dilation, groups=1)\n",
    "            self.depthwise = nn.Conv2d(in_channels=input_c*2, out_channels=K*input_c*2,\n",
    "                                   kernel_size=kernel_size, padding='same', stride=1,\n",
    "                                   dilation=dilation, groups=input_c*2)\n",
    "            self.pointwise_comp = nn.Conv2d(in_channels=K*input_c*2, out_channels=output_c,\n",
    "                                        kernel_size=1, padding='same', stride=1,\n",
    "                                        groups=1)\n",
    "            self.batchn11 = nn.BatchNorm2d(input_c*2, momentum=0.001)\n",
    "            self.batchn12 = nn.BatchNorm2d(input_c*2, momentum=0.001)\n",
    "            self.batchn13 = nn.BatchNorm2d(output_c, momentum=0.001)\n",
    "            \n",
    "            self.SqueezeAndExcitation = SqueezeAndExcitation(output_c)\n",
    "    \n",
    "    def forward(self, input):\n",
    "\n",
    "        if self.depth == True:\n",
    "\n",
    "            if self.s == 1:\n",
    "                # ResNet Block (Depth-seperable conv)\n",
    "                g1 = F.relu(self.batchn11(self.pointwise_exp(input)))\n",
    "                g1 = F.relu(self.batchn12(self.depthwise(g1)))\n",
    "                g1 = F.relu(self.batchn13(self.pointwise_comp(g1)))\n",
    "                g1 = self.SqueezeAndExcitation(g1)\n",
    "                # Concate\n",
    "                g1 = F.relu(g1 + input)\n",
    "                \n",
    "                return g1\n",
    "\n",
    "            elif self.s == 2:\n",
    "                # padding = (input.size(2)+self.k_s-2)//2\n",
    "                # g1 = F.pad(g1, (padding, padding+1, padding, padding+1))\n",
    "                # ResNet Block (Depth-seperable conv)\n",
    "                g1 = F.relu(self.batchn21(self.conv_2(input)))\n",
    "                g1 = F.relu(self.batchn22(self.depthwise_2(g1)))\n",
    "                g1 = F.relu(self.batchn23(self.pointwise_comp_2(g1)))\n",
    "                # Concate\n",
    "                # convert to required channels\n",
    "                input = F.relu(self.batchn23(self.pointwise_concat(input)))\n",
    "                g1 = self.SqueezeAndExcitation(g1)\n",
    "                g1 = F.relu(g1 + input)\n",
    "                return g1\n",
    "\n",
    "            else:\n",
    "                print(\"Only Stride 1 or 2 allowed\")\n",
    "                return 1\n",
    "\n",
    "        else:\n",
    "            print(\"For Now only Depthwise allowed\")\n",
    "            return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ASPP(nn.Module):\n",
    "    def __init__(self, input_c=256, multigrid=(1, 2, 3), ratio=6, K=1):\n",
    "        super(ASPP, self).__init__()\n",
    "\n",
    "        self.multigrid = multigrid\n",
    "        self.b_1 = nn.Conv2d(in_channels=input_c, out_channels=K*input_c,\n",
    "                             kernel_size=(3, 3), padding='same', stride=1,\n",
    "                             dilation=self.multigrid[0]*ratio, groups=input_c)\n",
    "        self.b_2 = nn.Conv2d(in_channels=input_c, out_channels=K*input_c,\n",
    "                             kernel_size=(3, 3), padding='same', stride=1,\n",
    "                             dilation=self.multigrid[1]*ratio, groups=input_c)\n",
    "        self.b_3 = nn.Conv2d(in_channels=input_c, out_channels=K*input_c,\n",
    "                             kernel_size=(3, 3), padding='same', stride=1,\n",
    "                             dilation=self.multigrid[2]*ratio, groups=input_c)\n",
    "        self.b_1x1 = nn.Conv2d(in_channels=input_c, out_channels=K*input_c,\n",
    "                               kernel_size=(1, 1), padding='same', stride=1,\n",
    "                               dilation=self.multigrid[0]*ratio, groups=input_c)\n",
    "\n",
    "        self.pooling_conv = nn.Conv2d(in_channels=input_c, out_channels=input_c,\n",
    "                                      kernel_size=(1, 1), padding='same', stride=1,\n",
    "                                      groups=1)\n",
    "        self.out_pool = nn.Upsample(size = (32,32), mode='bilinear')\n",
    "        self.final_conv = nn.Conv2d(in_channels=input_c*5, out_channels=input_c,\n",
    "                                    kernel_size=(1, 1), padding='same', stride=1,\n",
    "                                    groups=1)\n",
    "        self.batchn1 = nn.BatchNorm2d(input_c)\n",
    "        self.batchn2 = nn.BatchNorm2d(input_c)\n",
    "        self.batchn3 = nn.BatchNorm2d(input_c)\n",
    "        self.batchn4 = nn.BatchNorm2d(input_c)\n",
    "        self.batchn5 = nn.BatchNorm2d(input_c)\n",
    "        self.batchn6 = nn.BatchNorm2d(input_c)\n",
    "        self.batchn7 = nn.BatchNorm2d(input_c)\n",
    "        self.SqueezeAndExcitation = SqueezeAndExcitation(input_c*5)\n",
    "    def forward(self, inputs):\n",
    "        b_1 = F.relu(self.batchn1(self.b_1(inputs)))\n",
    "        b_2 = F.relu(self.batchn2(self.b_2(inputs)))\n",
    "        b_3 = F.relu(self.batchn3(self.b_3(inputs)))\n",
    "        b_1x1 = F.relu(self.batchn4(self.b_1x1(inputs)))\n",
    "        \n",
    "        k = b_3.size(2)\n",
    "        n = b_3.size(3)\n",
    "        out_pool = F.adaptive_avg_pool2d(inputs, 1)\n",
    "        out_pool = out_pool.reshape(inputs.size(0), inputs.size(1), 1, 1)\n",
    "        out_pool = F.relu(self.batchn6(self.pooling_conv(out_pool)))   \n",
    "        # out_pool = self.out_pool(out_pool)\n",
    "\n",
    "        x = torch.cat((b_1, b_2, b_3, b_1x1, out_pool.repeat(1,1,k,n)), dim=1)\n",
    "        x = self.SqueezeAndExcitation(x)\n",
    "        x = F.relu(self.batchn7(self.final_conv(x)))\n",
    "    \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepLab(nn.Module):\n",
    "    def __init__(self, input_c, num_classes, device, num_block=3):\n",
    "        super(DeepLab, self).__init__()\n",
    "        self.ks = 64\n",
    "        self.final_input = (self.ks*(num_block+1))+(self.ks)\n",
    "        self.c_1 = nn.Conv2d(in_channels=input_c, out_channels=self.ks,\n",
    "                             kernel_size=(7, 7), padding=3, stride=2)\n",
    "        self.mp_1 = nn.MaxPool2d(kernel_size=(3, 3), stride=(2, 2), padding=1)\n",
    "        self.block_1 = BlockResNet(self.ks, self.ks, device=device)\n",
    "        self.block_1_2 = BlockResNet(\n",
    "            self.ks, self.ks*2, stride=2, device=device)\n",
    "        self.block_2 = BlockResNet(self.ks*2, self.ks*2, device=device)\n",
    "        self.block_2_2 = BlockResNet(\n",
    "            self.ks*2, self.ks*4, stride=2, device=device)\n",
    "        self.block_3 = BlockResNet(\n",
    "            self.ks*4, self.ks*4, dilation=2, device=device)\n",
    "        self.assp = ASPP()\n",
    "        self.upsample = nn.Upsample(scale_factor=4)\n",
    "        self.conv_3x3 = nn.Conv2d(in_channels=self.final_input, out_channels=num_classes,\n",
    "                                  stride=1, kernel_size=(3, 3), padding='same')\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        x = self.c_1(inputs)\n",
    "        x = self.mp_1(x)\n",
    "\n",
    "        for _ in range(3):   # Block 1\n",
    "            x = self.block_1(x)\n",
    "        skip = x\n",
    "        x = self.block_1_2(x)\n",
    "\n",
    "        for _ in range(3):   # Block 2\n",
    "            x = self.block_2(x)\n",
    "        x = self.block_2_2(x)\n",
    "\n",
    "        for _ in range(3):  # Block 3\n",
    "            x = self.block_3(x)\n",
    "        x = self.assp(x)\n",
    "\n",
    "        x = self.upsample(x)\n",
    "        x = torch.cat([skip, x], dim=1)\n",
    "\n",
    "        x = self.conv_3x3(x)\n",
    "        x = self.softmax(x)\n",
    "        x = self.upsample(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function Definitions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checkpoint(state, filename,run_name,extension):\n",
    "    x = os.path.join(filename,run_name+extension)\n",
    "    if os.path.exists(x):\n",
    "        torch.save(state, x)\n",
    "        print(\"Saving Checkpoint\")\n",
    "        return\n",
    "    else:\n",
    "        with open(x, 'w') as fp:\n",
    "            fp.close()\n",
    "        # If model is running for first time then Save\n",
    "        torch.save(state, x)\n",
    "        print(\"Saving Checkpoint\")\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def meanIOU(true, predicted):\n",
    "    # Calculates mIOU for a mini-batch\n",
    "    predicted_arr = predicted.clone().detach().cpu().numpy().argmax(1)\n",
    "    groundt = true.clone().detach().cpu().numpy()\n",
    "    intersection = np.logical_and(groundt, predicted_arr).sum()\n",
    "    union = np.logical_or(groundt, predicted_arr).sum()\n",
    "\n",
    "    iou_score = intersection / union\n",
    "\n",
    "    return iou_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accuracy check function\n",
    "def check_accuracy(loader, model):\n",
    "    if loader.dataset.train:\n",
    "        print(\"testing on training data\")\n",
    "    else:\n",
    "        print(\"Testing on Testing data\")\n",
    "    correct = 0\n",
    "    iou = 0\n",
    "    num_samples = 0\n",
    "    model.eval()  # We're telling model to shift to eval mode\n",
    "\n",
    "    with torch.no_grad():   # we dont want the model to calculate the graidents\n",
    "        # just the outputs are required as model is already trained\n",
    "        start1 = time.time()\n",
    "        for x, y in loader:\n",
    "            x = x.to(mps_device)\n",
    "            y = y.to(mps_device)\n",
    "            num_samples += x.size(0)*y.size(1)*y.size(2)\n",
    "            scores = model(x)\n",
    "            preds = scores.clone().detach().max(1)[1]\n",
    "            correct += preds.eq(y).sum().item()\n",
    "            iou += meanIOU(y,scores)\n",
    "            \n",
    "\n",
    "        # end = time.time()\n",
    "        # print(f\"epoch {epochs} time : {start-end}\")\n",
    "        epoch_accuracy = round((correct / (num_samples))*100,2)\n",
    "        print(\"val_accuracy : \", epoch_accuracy)\n",
    "        model.train()\n",
    "        return round(epoch_accuracy, 2), iou/len(loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_root = 'dataset/images/train/'\n",
    "train_mask_root = 'dataset/mask/train/'\n",
    "\n",
    "test_root = 'dataset/images/val/'\n",
    "test_mask_root = 'dataset/mask/val/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([  0,  29,  76, 149], dtype=torch.uint8)\n",
      "torch.Size([1, 720, 1280])\n",
      "<class 'torch.Tensor'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# RANDOM TEST\n",
    "im1 = os.path.join(train_root, 'img_resize_0.png')\n",
    "mas1 = os.path.join(train_mask_root, 'img_resize_133.png')\n",
    "\n",
    "x = torchvision.io.read_image(mas1, mode=ImageReadMode.GRAY)\n",
    "plt.imshow(x.permute(1, 2, 0))\n",
    "plt.show()\n",
    "\n",
    "print(torch.unique(x))\n",
    "print(x.shape)\n",
    "print(type(x))\n",
    "os.path.isfile(train_root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To remove _mask at the end of each file to make it cosistent while sorting\n",
    "for filename in os.listdir(train_mask_root):\n",
    "    file_path = os.path.join(train_mask_root, filename)\n",
    "\n",
    "    if os.path.isfile(file_path):\n",
    "        file_name, file_extension = os.path.splitext(filename)\n",
    "\n",
    "        # Split the file name into words and remove the last 3 words\n",
    "        if file_name.endswith('_mask'):\n",
    "            new_file_name = file_name[:-5] + file_extension\n",
    "            new_file_path = os.path.join(train_mask_root, new_file_name)\n",
    "            os.rename(file_path, new_file_path)\n",
    "            print(f\"Renamed: {filename} -> {new_file_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class sat_dataset(Dataset):\n",
    "    def __init__(self, root, mask_root, inp_shape, transform=None, interpolation=IM.BILINEAR, train=True):\n",
    "        self.images = os.listdir(root)\n",
    "        self.masks = os.listdir(mask_root)\n",
    "        self.train_bool = train\n",
    "        self.root = root\n",
    "        self.mask_root = mask_root\n",
    "        self.transform = transform\n",
    "        self.inp_shape = inp_shape\n",
    "        self.interpolation = interpolation\n",
    "        self.resize = transforms.Resize(size=self.inp_shape, interpolation=self.interpolation,\n",
    "                                        antialias=True)\n",
    "        for i in self.images:\n",
    "            if (i == '.DS_Store'):\n",
    "                self.images.remove('.DS_Store')\n",
    "        for i in self.masks:\n",
    "            if (i == '.DS_Store'):\n",
    "                self.masks.remove('.DS_Store')\n",
    "        self.images.sort()\n",
    "        self.masks.sort()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def train(self) -> bool:\n",
    "        return self.train_bool\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "\n",
    "        img_path = os.path.join(self.root, self.images[index])\n",
    "        img = torchvision.io.read_image(img_path, mode=ImageReadMode.RGB)\n",
    "        # Always use RGB as mode or else it ouputs 4 dim sometimes\n",
    "        img_path = os.path.join(self.mask_root, self.masks[index])\n",
    "        mask = torchvision.io.read_image(img_path, mode=ImageReadMode.GRAY)\n",
    "\n",
    "        # One-Hot encoding mask\n",
    "        mask = mask.numpy()\n",
    "        # Define the mapping from original values to new values\n",
    "        value_mapping = {0: 0, 76: 1, 149: 2, 29: 3}  # 149 -> yellow\n",
    "        # 76-> green_blue\n",
    "        # 29 -> blue\n",
    "        # Apply the mapping to the entire image array\n",
    "        mask = np.vectorize(value_mapping.get)(mask)\n",
    "        mask = torch.tensor(mask, dtype=torch.long)\n",
    "        # print(torch.unique(mask))\n",
    "        # print(mask.shape)\n",
    "\n",
    "        img = img/255.0\n",
    "\n",
    "        # img = img.astype(np.float32)\n",
    "        # mask = mask.astype(np.float32)\n",
    "\n",
    "        if (self.transform):\n",
    "            img = self.transform(img)\n",
    "            mask = self.transform(mask)\n",
    "\n",
    "        img = self.resize(img)\n",
    "        mask = self.resize(mask)\n",
    "        mask = mask.squeeze(0)\n",
    "\n",
    "        return img, mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = 4\n",
    "learning_rate = 0.001\n",
    "batch_size = 10\n",
    "num_epochs = 100\n",
    "filename = 'model_runs'\n",
    "run_name = 'DeepLab_new'\n",
    "extension = '.pth.tar'\n",
    "inputH = 512\n",
    "inputW = 512\n",
    "\n",
    "inp_dim = (inputH, inputW)\n",
    "inputC = 3\n",
    "class_weight = torch.tensor([0.29, 1.2, 0.7, 20], device=torch.device('mps'))\n",
    "mps_device = torch.device('mps')\n",
    "model1 = DeepLab(input_c=inputC, num_classes=classes, device=mps_device)\n",
    "model = model1.to(mps_device)\n",
    "metrics = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "sat_train_data = sat_dataset(root=train_root, mask_root=train_mask_root,\n",
    "                             inp_shape=inp_dim,\n",
    "                             #  transform=transforms.ToTensor()\n",
    "                             )\n",
    "train_loader = DataLoader(dataset=sat_train_data,\n",
    "                          shuffle=True, batch_size=batch_size)\n",
    "\n",
    "sat_test_data = sat_dataset(root=test_root, mask_root=test_mask_root,\n",
    "                             inp_shape=inp_dim,train=False\n",
    "                             #  transform=transforms.ToTensor()\n",
    "                             )\n",
    "test_loader = DataLoader(dataset=sat_test_data,\n",
    "                          shuffle=True, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (os.path.isfile(os.path.join(run_name+extension))):\n",
    "    prev_model = torch.load(os.path.join(run_name+extension))\n",
    "    model.load_state_dict(prev_model['state_dict'])\n",
    "    max_iou = prev_model['m_iou']\n",
    "else:\n",
    "    max_iou = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_acc_l = []\n",
    "train_acc_l = []\n",
    "loss_l = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epochs in range(num_epochs):\n",
    "    # start = time.time()\n",
    "    num_samples = 0\n",
    "    iou = 0\n",
    "    final_loss = 0\n",
    "    correct = 0\n",
    "    loop = tqdm(train_loader)\n",
    "\n",
    "    for batch_idx, (train, targets) in enumerate(loop):\n",
    "        data = train.to(mps_device)\n",
    "        targets = targets.to(mps_device)  # now we have loaded the data\n",
    "\n",
    "        num_samples += data.size(0)  # Clac num_samples to calc average loss\n",
    "        scores = model(data)\n",
    "        \n",
    "        loss = metrics(scores, targets)\n",
    "        final_loss += loss.item()  # to calculate average loss\n",
    "        \n",
    "        # gradient descent\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # OVerall acc\n",
    "        preds = scores.clone().detach().max(1)[1]\n",
    "        correct += preds.eq(targets).sum().item()\n",
    "    # end = time.time()\n",
    "    # print(f\"epoch {epochs} time : {start-end}\")\n",
    "    final_loss = final_loss/num_samples\n",
    "    epoch_accuracy = round((correct / (num_samples*inputH*inputW))*100, 3)\n",
    "    scheduler.step(final_loss)\n",
    "    val_acc, iou = check_accuracy(test_loader, model)\n",
    "    \n",
    "    val_acc_l.append(val_acc)\n",
    "    train_acc_l.append(epoch_accuracy)\n",
    "    loss_l.append(final_loss)\n",
    "    \n",
    "    print(\"train_accuracy : \", epoch_accuracy)\n",
    "    print(f\"IoU : \", iou)\n",
    "    print(f\"Loss {epochs} : {final_loss:5f}\")\n",
    "\n",
    "    if (iou > max_iou):\n",
    "        max_iou = iou\n",
    "        min_loss = final_loss\n",
    "        checkpoint = {'state_dict': model.state_dict(),\n",
    "                      'optimizer': optimizer.state_dict(),\n",
    "                      'val_accuracy': val_acc,\n",
    "                      'train_accuracy': epoch_accuracy,\n",
    "                      'epoch': epochs,\n",
    "                      'loss': final_loss,\n",
    "                      'm_iou':iou}\n",
    "        save_checkpoint(checkpoint,'',run_name,extension)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom Test data\n",
    "sat_test_data = sat_dataset(test_root, test_mask_root, train=False,\n",
    "                            inp_shape=inp_dim)\n",
    "# transform=transforms.ToTensor())\n",
    "test_loader = DataLoader(sat_test_data, shuffle=True, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_item_test(i):\n",
    "\n",
    "    x, y = sat_test_data.__getitem__(i)\n",
    "    x = x.unsqueeze(0)  # as there is no batch size\n",
    "    y = y.unsqueeze(0)  # add an extra dimension to fit the model\n",
    "    x = x.to(mps_device)\n",
    "    y = y.to(mps_device)\n",
    "\n",
    "    scores = model(x)\n",
    "    _, predictions = scores.max(1)\n",
    "\n",
    "    # making shape compatible for numpy operation\n",
    "    predictions = predictions.cpu().numpy()\n",
    "    y = y.cpu().numpy()\n",
    "    shape = (1, inputH, inputW)  # shape of output of model\n",
    "    y = np.resize(y, (shape[1], shape[2], shape[0]))\n",
    "    predictions = np.resize(predictions, (shape[1], shape[2], shape[0]))\n",
    "\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(10, 5))\n",
    "\n",
    "    # Plot the first image on the left subplot\n",
    "    # Adjust colormap and limits as needed\n",
    "    axes[0].imshow(y, cmap='viridis', vmin=0, vmax=3)\n",
    "    axes[0].set_title(\"Ground Truth\")\n",
    "\n",
    "    # Plot the second image on the right subplot\n",
    "    # Adjust colormap and limits as needed\n",
    "    axes[1].imshow(predictions, cmap='viridis', vmin=0, vmax=3)\n",
    "    axes[1].set_title(\"Prediction\")\n",
    "\n",
    "    # Remove axis labels and ticks\n",
    "    for ax in axes:\n",
    "        ax.axis('off')\n",
    "\n",
    "    # Display the side-by-side images\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_item_test(225)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.autograd.grad_mode.no_grad at 0x16a3748e0>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# preparing model for inference\n",
    "prev_model = torch.load('DeepLab_new.pth.tar')\n",
    "model.load_state_dict(prev_model['state_dict'])\n",
    "model.eval()\n",
    "model.to(mps_device)\n",
    "torch.no_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testing on training data\n",
      "val_accuracy :  93.42\n",
      "MeanIoU on Test : 0.8133876366123717\n"
     ]
    }
   ],
   "source": [
    "acc, iou = check_accuracy(test_loader, model)\n",
    "print(f\"MeanIoU on Test : {iou}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Register count_convNd() for <class 'torch.nn.modules.conv.Conv2d'>.\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.pooling.MaxPool2d'>.\n",
      "[INFO] Register count_normalization() for <class 'torch.nn.modules.batchnorm.BatchNorm2d'>.\n",
      "[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.\n",
      "[INFO] Register count_upsample() for <class 'torch.nn.modules.upsampling.Upsample'>.\n",
      "[INFO] Register count_softmax() for <class 'torch.nn.modules.activation.Softmax'>.\n",
      "Computational Complexity :  4283387904.0\n"
     ]
    }
   ],
   "source": [
    "from thop import profile\n",
    "\n",
    "input = torch.randn(1, 3, 512, 512).to(mps_device)\n",
    "macs, params = profile(model, inputs=(input, ))\n",
    "print(\"Computational Complexity : \", macs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Error Diagonise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "element 0 does not have grad fnc :\n",
    "problem with the way you're processing targets of the images\n",
    "due to which loss function is always 0\n",
    "\n",
    "Stack must have equal shape [3,512,512] at entry 0 and [4,512,512] at entry 8:\n",
    "THis is due to DataLoader, preobably the way you load the image to array, some images channels are not uniform\n",
    "use IMreadmode everytime to load image\n",
    "\n",
    "pytorch not compatible with int use long:\n",
    "pytorch uses only float32 or long so always convert to one of these formats while running model\n",
    "\n",
    "Loss errors:\n",
    "predictions must be (bathc, num-class, height, width)\n",
    "targets must be (batch, height, width)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
